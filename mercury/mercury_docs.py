#!/usr/bin/env python


import os, sys

SCRIPT_NAMES = ['beekeeper',
'cfiltr',
'chunkr',
'collapsr',
'countup',
'csvstack',
'datetimecalc',
'dgenr8',
'expath',
'fgate',
'ifvar',
'jfiltr',
'jsonfile2csv',
'jsonL2nvp',
'normalize']



beekeeper = """
______________________________________________

+++  Mercury script: beekeeper +++
______________________________________________

Usage:
    beekeeper [-d] --config <config_file> --target <api_target> [--r-params=<n:v>,...] [--t-params=<n:v>,...] [--macro-args=<n:v>,...]
    beekeeper [-d] -b --config <config_file> --target <api_target> --outfile <filename> [--r-params=<n:v>,...] [--t-params=<n:v>,...] [--macro-args=<n:v>,...]
    beekeeper --config <config_file> --list

Options:
    -d --debug      execute in debug mode (dump headers and template values)
    -b --bytes      return request data as bytes (as opposed to string)    


beekeeper is a command-line utility for interacting with HTTP API's.
It is driven by a YAML configuration file whose core metaphor is the <target>. 

Beekeeper targets are defined in the following structure within the YAML file:

targets:
    <target_name>:
      url: <url_endpoint>
      method: <http_method>
      headers: ## optional                                  
          <header_name>: <header_value>
          ...

      request_params:
        - name: <parameter_name>
          value: <parameter_value>
        ...

So that issuing the command

beekeeper --config <yamlfile> --target <target_name>

will issue the corresponding HTTP request to the designated URL in the configuration.
A request can have an arbitrary number of header fields and request parameters.



"""



cfiltr = """
______________________________________________

+++  Mercury script: cfiltr +++
______________________________________________

Usage:
    cfilter [-d <delim>] --accept-to <filename> --filter <module.function> <source_file> [--params=<name:value>...] [--limit=<limit>]
    cfilter [-d <delim>] --reject-to <filename> --filter <module.function> <source_file> [--params=<name:value>...] [--limit=<limit>]
    cfilter --config <configfile> --setup <setup_name> --source <source_file> [--params=<name:value>...] [--limit=<limit>]

Options:
    -d  --delimiter set the delimiter to the specified character (comma is default)


cfilter (CSV filter) filters a source CSV file <source_file> by processing each record through 
a filter function specified by <module.function>. 

The filter function's signature is:

filter_func(record: dict, line: str, service_registry=None, **filter_args) -> bool

where <line> is the raw CSV record and <record> is its dictionary form. The filter function is only passed a 
service registry iff cfilter is called with the --config option set; then the service registry will contain
live instances of all service objects specified in <configfile>.


if the filter function returns True, the record will be deemed as ACCEPTED. If False, the record will be deemed as REJECTED.

When the --accept-to option is set, ACCEPTED records are written to <filename> and REJECTED records are written to standard out.

When the --reject-to option is set, REJECTED records are written to <filename> and ACCEPTED records are written to standard out.

The source CSV file must contain a header; if it does not, cfilter's behavior is undefined.



"""



chunkr = """
______________________________________________

+++  Mercury script: chunkr +++
______________________________________________

Usage:
    chunkr --filenames <listfile> --src-dir <directory> --chunks <count> --pfx <outfile_prefix> --ext <extension> [-t <outdir>] [limit=<max_files>]
    chunkr --records <datafile> --chunks <count> --pfx <outfile_prefix> --ext <extension> [-t <outdir>] [limit=<max_records>]

Options:
    -t --target-dir  send chunked datafiles to this directory


chunkr splits a list of strings and divides it into N chunks, where N is specified by 

--chunks <count>

It has two modes: filename-mode (selected by passing --filenames <listfile>) and record-mode
(selected by passing --records <datafile>).

In filename mode, it will treat the <listfile> parameter as a meta-list; that is, it will compile 
its master list of records to be chunked by reading every filename specified in <listfile>.

In record mode, it will treat <datafile> as its master list of records to be chunked.

Each chunkfile generated by chunkr will be named in the format <outfile-prefix>_N.<extension>. 
By default it will write chunkfiles to the current directory, but can write them to the directory specified
by the optional <outdir> parameter.

The optional limit=<N> parameter limits it to processing N input records.



"""



collapsr = """
______________________________________________

+++  Mercury script: collapsr +++
______________________________________________

Usage:
    collapsr --listfile <file>
    collapsr -s

Options:
    -s --stdin    read records from standard input


collapsr takes a list of values from <file> or standard input and collapses them
down to a set of unique values. (Think SELECT DISTINCT, but for textfiles.)



"""



countup = """
______________________________________________

+++  Mercury script: countup +++
______________________________________________

Usage:
    countup --from <start_number> --to <number> [--zpad <length>]
    countup --to <number> [-z] [--zpad <length>]
    countup -s [-z] [--from <start_number>] [--zpad <length>]

Options:
    -z --zero-based  count from zero instead of one
    -s --stdin  count the number of lines sent via stdin


countup emits a list of consecutive integers (one per line) from zero or <start_number>
to <number>, inclusive. The integers can be optionally zero-padded by passing the optional

--zpad <length>

parameter.

Running countup with the -s (--stdin) parameter will cause it to emit a consecutive integer 
for each line sent to countup via standard input, starting with zero or <start_number>. This makes
it easy to (for example) generate ROWIDs, or line numbers for a text file.



"""



csvstack = """
______________________________________________

+++  Mercury script: csvstack +++
______________________________________________

Usage:
    csvstack --listfile <filename> 
    csvstack --files <file1>...


csvstack concatenates a list of CSV files to standard output, only printing the header of the first
file in the list.

In --listfiles mode, it will read data from each CSV file listed in <filename>.
In --files mode, it will read data from each CSV file passed on the command line as a comma-separated list
(no spaces).

Note that csvstack assumes that each file will contain a single-line header. It is agnostic 
with respect to delimiters.



"""



datetimecalc = """
______________________________________________

+++  Mercury script: datetimecalc +++
______________________________________________

Usage:
    datetimecalc --days <num_days> (--before | --from) (today | now)  


datetimecalc yields a single date or datetime value <num_days> days into the future or the past.
The --before option is past-looking; the --from option is future-looking. So that


datetimecalc --days 1 --before today

gives yesterday's date, and

datetimecalc --days 1 --after today

gives tomorrow's date.


Pass: 

today 

as the last parameter and it will yield a date; pass: 

now 

and it will yield a datetime.



"""



dgenr8 = """
______________________________________________

+++  Mercury script: dgenr8 +++
______________________________________________

Usage:
    dgenr8 --plugin-module <module> --sql --schema <schema> --dim-table <tablename> --columns <columns>... [--limit=<limit>]
    dgenr8 --plugin-module <module> --sqlmulti --schema <schema> --dim-table <tablename> --columns <columns>... [--limit=<limit>]
    dgenr8 --plugin-module <module> --csv --delimiter <delimiter> --columns <columns>... [--limit=<limit>]
    dgenr8 --generator <module.function> --sql --schema <schema> --dim-table <tablename> --columns <columns>... [--limit=<limit>]
    dgenr8 --generator <module.function> --sqlmulti --schema <schema> --dim-table <tablename> --columns <columns>... [--limit=<limit>]
    dgenr8 --generator <module.function> --csv --delimiter <delimiter> --columns <columns>... [--limit=<limit>]


dgenr8 (dimension table generator) generates SQl insert statements or CSV records 
to populate OLAP star-schema dimension tables.



"""



expath = """
______________________________________________

+++  Mercury script: expath +++
______________________________________________

Usage:
    expath --listfile <file>
    expath -s

Options:
    -s --stdin    read records from standard input


expath (extract path) takes a list of fully-qualified file references 
(from <file> if the --listfile option is set, or from standard input if -s is set)
and yields the paths only, stripping away the filename and the trailing slash.

The input path(s) need not be valid; that is, they CAN point to nonexistent files.



"""



fgate = """
______________________________________________

+++  Mercury script: fgate +++
______________________________________________

Usage:
    fgate.py --config <configfile> --gate <gate> <source_file>
    fgate.py --gate-module <module> --gate-function <func> --format (csv | json) <source_file> [--delimiter=<delimiter>]
    fgate.py --gate-module <module> --service_module <svc-module> --gate-function <func> --format (csv | json) <source_file> [--delimiter=<delimiter>]


fgate (file gate): a data utility for all-or-nothing validation of a datafile.

fgate is intended to run against a source datafile (supported formats are csv and json). The user
is to supply a "gate function" which receives an open file handle and can step through the file's
contents, returning True if the file should pass (a "GO" condition) and False if it should fail
(a "NO-GO" condition).

In the case of a GO condition, fgate will emit the name of the datafile to standard out. In the case
of a NO-GO condition, fgate will return an empty string.



"""



ifvar = """
______________________________________________

+++  Mercury script: ifvar +++
______________________________________________

Usage: 
    ifvar <env_var> --str <output_string> [--argprefix]
    ifvar <env_var> --token <vartoken> --t <output_string> [--argprefix]
    ifvar <env_var> --expr <py_expr> 

Options:
    -t --template   Use the output string as a template, subbing in the specified var if present
    -a --argprefix  Prepend a double-dash to the output string


ifvar prints a value to standard out if the environment variable <env_var> is set.

if the --str option is set, it will print <output_string> (preceded with a double-dash if
--argprefix is passed as well).

if --token is set, it will print the template <output_string>, interpolating the value of
the specified env var for <vartoken> in the template. So if the env var HOME is set,
the command

ifvar HOME --token % --t %/bin

will yield:

<your home directory>/bin

if --expr is set, it will execute the quoted python expression <py_expr>.



"""



jfiltr = """
______________________________________________

+++  Mercury script: jfiltr +++
______________________________________________

Usage:
    jfilter --accept-to <filename> --filter <module.function> <source_file> [--params=<name:value>...] [--limit=<limit>]
    jfilter --reject-to <filename> --filter <module.function> <source_file> [--params=<name:value>...] [--limit=<limit>]
    jfilter --config <configfile> --setup <setup_name> --source <source_file> [--params=<name:value>...] [--limit=<limit>]

Options:
    -c  --csv  process records as CSV


jfilter (JSON filter) filters a source JSON file <source_file> by processing each record through 
a filter function specified by <module.function>. 

The filter function's signature is:

filter_func(record: dict, line: str, service_registry=None, **filter_args) -> bool

where <line> is the raw JSON record and <record> is its dictionary form. The filter function is only passed a 
service registry iff jfilter is called with the --config option set; then the service registry will contain
live instances of all service objects specified in <configfile>.


If the filter function returns True, the record will be deemed as ACCEPTED. If False, the record will be deemed as REJECTED.

When the --accept-to option is set, ACCEPTED records are written to <filename> and REJECTED records are written to standard out.

When the --reject-to option is set, REJECTED records are written to <filename> and ACCEPTED records are written to standard out.



"""



jsonfile2csv = """
______________________________________________

+++  Mercury script: jsonfile2csv +++
______________________________________________

Usage:
    jsonfile2csv <jsonfile> --generator <module.class> --delimiter <delimiter> [--params=<name:value>...] [--limit=<max_records>]
    jsonfile2csv --generator <module.class> --delimiter <delimiter> [--params=<name:value>...] [--limit=<max_records>]


### UNDER CONSTRUCTION



"""



jsonL2nvp = """
______________________________________________

+++  Mercury script: jsonL2nvp +++
______________________________________________

Usage:
    jsonL2nvp --datafile <file> --key <key_field> --value <value_field>
    jsonL2nvp -s --key <key_field> --value <value_field>

Options:
    -s --stdin  read data from standard input


jsonL2nvp (JSONL to name-value pairs) transforms a set of JSONL records into an array of name/value pairs 
where name = source_record[<key_field>] and value = source_record[<value_field>].

So that if we start with a sample.json file containing:

{"first_name": "John", "last_name": "Smith"}
{"first_name":"Bob", "last_name":"Barker"}

and issue the command:

jsonL2nvp --datafile sample.json --key first_name --value last_name

we will receive the output:

[{"John": "Smith"}, {"Bob": "Barker"}]

jsonL2nvp reads its input records from <file> if the --datafile option is set, and
from standard input if the --stdin option is set.



"""



normalize = """
______________________________________________

+++  Mercury script: normalize +++
______________________________________________

Usage:
    normalize --datafile <file>
    normalize -s

Options:
    -s --stdin  read data from standard input


normalize takes a set of lines (from a file or standard input) and performs two transforms
on each line:

- replaces whitespace with underscores; and
- changes uppercase chars to lowercase.



"""


